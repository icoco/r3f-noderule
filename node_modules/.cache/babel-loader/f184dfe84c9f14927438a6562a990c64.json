{"ast":null,"code":"\"use strict\";\n\nvar __importDefault = this && this.__importDefault || function (mod) {\n  return mod && mod.__esModule ? mod : {\n    \"default\": mod\n  };\n};\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.tokenMatcher = exports.createTokenInstance = exports.EOF = exports.createToken = exports.hasTokenLabel = exports.tokenName = exports.tokenLabel = void 0;\n\nconst isString_1 = __importDefault(require(\"lodash/isString\"));\n\nconst has_1 = __importDefault(require(\"lodash/has\"));\n\nconst isUndefined_1 = __importDefault(require(\"lodash/isUndefined\"));\n\nconst lexer_public_1 = require(\"./lexer_public\");\n\nconst tokens_1 = require(\"./tokens\");\n\nfunction tokenLabel(tokType) {\n  if (hasTokenLabel(tokType)) {\n    return tokType.LABEL;\n  } else {\n    return tokType.name;\n  }\n}\n\nexports.tokenLabel = tokenLabel;\n\nfunction tokenName(tokType) {\n  return tokType.name;\n}\n\nexports.tokenName = tokenName;\n\nfunction hasTokenLabel(obj) {\n  return (0, isString_1.default)(obj.LABEL) && obj.LABEL !== \"\";\n}\n\nexports.hasTokenLabel = hasTokenLabel;\nconst PARENT = \"parent\";\nconst CATEGORIES = \"categories\";\nconst LABEL = \"label\";\nconst GROUP = \"group\";\nconst PUSH_MODE = \"push_mode\";\nconst POP_MODE = \"pop_mode\";\nconst LONGER_ALT = \"longer_alt\";\nconst LINE_BREAKS = \"line_breaks\";\nconst START_CHARS_HINT = \"start_chars_hint\";\n\nfunction createToken(config) {\n  return createTokenInternal(config);\n}\n\nexports.createToken = createToken;\n\nfunction createTokenInternal(config) {\n  const pattern = config.pattern;\n  const tokenType = {};\n  tokenType.name = config.name;\n\n  if (!(0, isUndefined_1.default)(pattern)) {\n    tokenType.PATTERN = pattern;\n  }\n\n  if ((0, has_1.default)(config, PARENT)) {\n    throw \"The parent property is no longer supported.\\n\" + \"See: https://github.com/chevrotain/chevrotain/issues/564#issuecomment-349062346 for details.\";\n  }\n\n  if ((0, has_1.default)(config, CATEGORIES)) {\n    // casting to ANY as this will be fixed inside `augmentTokenTypes``\n    tokenType.CATEGORIES = config[CATEGORIES];\n  }\n\n  (0, tokens_1.augmentTokenTypes)([tokenType]);\n\n  if ((0, has_1.default)(config, LABEL)) {\n    tokenType.LABEL = config[LABEL];\n  }\n\n  if ((0, has_1.default)(config, GROUP)) {\n    tokenType.GROUP = config[GROUP];\n  }\n\n  if ((0, has_1.default)(config, POP_MODE)) {\n    tokenType.POP_MODE = config[POP_MODE];\n  }\n\n  if ((0, has_1.default)(config, PUSH_MODE)) {\n    tokenType.PUSH_MODE = config[PUSH_MODE];\n  }\n\n  if ((0, has_1.default)(config, LONGER_ALT)) {\n    tokenType.LONGER_ALT = config[LONGER_ALT];\n  }\n\n  if ((0, has_1.default)(config, LINE_BREAKS)) {\n    tokenType.LINE_BREAKS = config[LINE_BREAKS];\n  }\n\n  if ((0, has_1.default)(config, START_CHARS_HINT)) {\n    tokenType.START_CHARS_HINT = config[START_CHARS_HINT];\n  }\n\n  return tokenType;\n}\n\nexports.EOF = createToken({\n  name: \"EOF\",\n  pattern: lexer_public_1.Lexer.NA\n});\n(0, tokens_1.augmentTokenTypes)([exports.EOF]);\n\nfunction createTokenInstance(tokType, image, startOffset, endOffset, startLine, endLine, startColumn, endColumn) {\n  return {\n    image,\n    startOffset,\n    endOffset,\n    startLine,\n    endLine,\n    startColumn,\n    endColumn,\n    tokenTypeIdx: tokType.tokenTypeIdx,\n    tokenType: tokType\n  };\n}\n\nexports.createTokenInstance = createTokenInstance;\n\nfunction tokenMatcher(token, tokType) {\n  return (0, tokens_1.tokenStructuredMatcher)(token, tokType);\n}\n\nexports.tokenMatcher = tokenMatcher;","map":{"version":3,"sources":["../../../src/scan/tokens_public.ts"],"names":[],"mappings":";;;;;;;;;;;;;AAAA,MAAA,UAAA,GAAA,eAAA,CAAA,OAAA,CAAA,iBAAA,CAAA,CAAA;;AACA,MAAA,KAAA,GAAA,eAAA,CAAA,OAAA,CAAA,YAAA,CAAA,CAAA;;AACA,MAAA,aAAA,GAAA,eAAA,CAAA,OAAA,CAAA,oBAAA,CAAA,CAAA;;AACA,MAAA,cAAA,GAAA,OAAA,CAAA,gBAAA,CAAA;;AACA,MAAA,QAAA,GAAA,OAAA,CAAA,UAAA,CAAA;;AAGA,SAAgB,UAAhB,CAA2B,OAA3B,EAA6C;AAC3C,MAAI,aAAa,CAAC,OAAD,CAAjB,EAA4B;AAC1B,WAAO,OAAO,CAAC,KAAf;AACD,GAFD,MAEO;AACL,WAAO,OAAO,CAAC,IAAf;AACD;AACF;;AAND,OAAA,CAAA,UAAA,GAAA,UAAA;;AAQA,SAAgB,SAAhB,CAA0B,OAA1B,EAA4C;AAC1C,SAAO,OAAO,CAAC,IAAf;AACD;;AAFD,OAAA,CAAA,SAAA,GAAA,SAAA;;AAIA,SAAgB,aAAhB,CACE,GADF,EACgB;AAEd,SAAO,CAAA,GAAA,UAAA,CAAA,OAAA,EAAS,GAAG,CAAC,KAAb,KAAuB,GAAG,CAAC,KAAJ,KAAc,EAA5C;AACD;;AAJD,OAAA,CAAA,aAAA,GAAA,aAAA;AAMA,MAAM,MAAM,GAAG,QAAf;AACA,MAAM,UAAU,GAAG,YAAnB;AACA,MAAM,KAAK,GAAG,OAAd;AACA,MAAM,KAAK,GAAG,OAAd;AACA,MAAM,SAAS,GAAG,WAAlB;AACA,MAAM,QAAQ,GAAG,UAAjB;AACA,MAAM,UAAU,GAAG,YAAnB;AACA,MAAM,WAAW,GAAG,aAApB;AACA,MAAM,gBAAgB,GAAG,kBAAzB;;AAEA,SAAgB,WAAhB,CAA4B,MAA5B,EAAgD;AAC9C,SAAO,mBAAmB,CAAC,MAAD,CAA1B;AACD;;AAFD,OAAA,CAAA,WAAA,GAAA,WAAA;;AAIA,SAAS,mBAAT,CAA6B,MAA7B,EAAiD;AAC/C,QAAM,OAAO,GAAG,MAAM,CAAC,OAAvB;AAEA,QAAM,SAAS,GAAmB,EAAlC;AACA,EAAA,SAAS,CAAC,IAAV,GAAiB,MAAM,CAAC,IAAxB;;AAEA,MAAI,CAAC,CAAA,GAAA,aAAA,CAAA,OAAA,EAAY,OAAZ,CAAL,EAA2B;AACzB,IAAA,SAAS,CAAC,OAAV,GAAoB,OAApB;AACD;;AAED,MAAI,CAAA,GAAA,KAAA,CAAA,OAAA,EAAI,MAAJ,EAAY,MAAZ,CAAJ,EAAyB;AACvB,UACE,kDACA,8FAFF;AAID;;AAED,MAAI,CAAA,GAAA,KAAA,CAAA,OAAA,EAAI,MAAJ,EAAY,UAAZ,CAAJ,EAA6B;AAC3B;AACA,IAAA,SAAS,CAAC,UAAV,GAA4B,MAAM,CAAC,UAAD,CAAlC;AACD;;AAED,GAAA,GAAA,QAAA,CAAA,iBAAA,EAAkB,CAAC,SAAD,CAAlB;;AAEA,MAAI,CAAA,GAAA,KAAA,CAAA,OAAA,EAAI,MAAJ,EAAY,KAAZ,CAAJ,EAAwB;AACtB,IAAA,SAAS,CAAC,KAAV,GAAkB,MAAM,CAAC,KAAD,CAAxB;AACD;;AAED,MAAI,CAAA,GAAA,KAAA,CAAA,OAAA,EAAI,MAAJ,EAAY,KAAZ,CAAJ,EAAwB;AACtB,IAAA,SAAS,CAAC,KAAV,GAAkB,MAAM,CAAC,KAAD,CAAxB;AACD;;AAED,MAAI,CAAA,GAAA,KAAA,CAAA,OAAA,EAAI,MAAJ,EAAY,QAAZ,CAAJ,EAA2B;AACzB,IAAA,SAAS,CAAC,QAAV,GAAqB,MAAM,CAAC,QAAD,CAA3B;AACD;;AAED,MAAI,CAAA,GAAA,KAAA,CAAA,OAAA,EAAI,MAAJ,EAAY,SAAZ,CAAJ,EAA4B;AAC1B,IAAA,SAAS,CAAC,SAAV,GAAsB,MAAM,CAAC,SAAD,CAA5B;AACD;;AAED,MAAI,CAAA,GAAA,KAAA,CAAA,OAAA,EAAI,MAAJ,EAAY,UAAZ,CAAJ,EAA6B;AAC3B,IAAA,SAAS,CAAC,UAAV,GAAuB,MAAM,CAAC,UAAD,CAA7B;AACD;;AAED,MAAI,CAAA,GAAA,KAAA,CAAA,OAAA,EAAI,MAAJ,EAAY,WAAZ,CAAJ,EAA8B;AAC5B,IAAA,SAAS,CAAC,WAAV,GAAwB,MAAM,CAAC,WAAD,CAA9B;AACD;;AAED,MAAI,CAAA,GAAA,KAAA,CAAA,OAAA,EAAI,MAAJ,EAAY,gBAAZ,CAAJ,EAAmC;AACjC,IAAA,SAAS,CAAC,gBAAV,GAA6B,MAAM,CAAC,gBAAD,CAAnC;AACD;;AAED,SAAO,SAAP;AACD;;AAEY,OAAA,CAAA,GAAA,GAAM,WAAW,CAAC;AAAE,EAAA,IAAI,EAAE,KAAR;AAAe,EAAA,OAAO,EAAE,cAAA,CAAA,KAAA,CAAM;AAA9B,CAAD,CAAjB;AACb,CAAA,GAAA,QAAA,CAAA,iBAAA,EAAkB,CAAC,OAAA,CAAA,GAAD,CAAlB;;AAEA,SAAgB,mBAAhB,CACE,OADF,EAEE,KAFF,EAGE,WAHF,EAIE,SAJF,EAKE,SALF,EAME,OANF,EAOE,WAPF,EAQE,SARF,EAQmB;AAEjB,SAAO;AACL,IAAA,KADK;AAEL,IAAA,WAFK;AAGL,IAAA,SAHK;AAIL,IAAA,SAJK;AAKL,IAAA,OALK;AAML,IAAA,WANK;AAOL,IAAA,SAPK;AAQL,IAAA,YAAY,EAAQ,OAAQ,CAAC,YARxB;AASL,IAAA,SAAS,EAAE;AATN,GAAP;AAWD;;AArBD,OAAA,CAAA,mBAAA,GAAA,mBAAA;;AAuBA,SAAgB,YAAhB,CAA6B,KAA7B,EAA4C,OAA5C,EAA8D;AAC5D,SAAO,CAAA,GAAA,QAAA,CAAA,sBAAA,EAAuB,KAAvB,EAA8B,OAA9B,CAAP;AACD;;AAFD,OAAA,CAAA,YAAA,GAAA,YAAA","sourceRoot":"","sourcesContent":["\"use strict\";\nvar __importDefault = (this && this.__importDefault) || function (mod) {\n    return (mod && mod.__esModule) ? mod : { \"default\": mod };\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.tokenMatcher = exports.createTokenInstance = exports.EOF = exports.createToken = exports.hasTokenLabel = exports.tokenName = exports.tokenLabel = void 0;\nconst isString_1 = __importDefault(require(\"lodash/isString\"));\nconst has_1 = __importDefault(require(\"lodash/has\"));\nconst isUndefined_1 = __importDefault(require(\"lodash/isUndefined\"));\nconst lexer_public_1 = require(\"./lexer_public\");\nconst tokens_1 = require(\"./tokens\");\nfunction tokenLabel(tokType) {\n    if (hasTokenLabel(tokType)) {\n        return tokType.LABEL;\n    }\n    else {\n        return tokType.name;\n    }\n}\nexports.tokenLabel = tokenLabel;\nfunction tokenName(tokType) {\n    return tokType.name;\n}\nexports.tokenName = tokenName;\nfunction hasTokenLabel(obj) {\n    return (0, isString_1.default)(obj.LABEL) && obj.LABEL !== \"\";\n}\nexports.hasTokenLabel = hasTokenLabel;\nconst PARENT = \"parent\";\nconst CATEGORIES = \"categories\";\nconst LABEL = \"label\";\nconst GROUP = \"group\";\nconst PUSH_MODE = \"push_mode\";\nconst POP_MODE = \"pop_mode\";\nconst LONGER_ALT = \"longer_alt\";\nconst LINE_BREAKS = \"line_breaks\";\nconst START_CHARS_HINT = \"start_chars_hint\";\nfunction createToken(config) {\n    return createTokenInternal(config);\n}\nexports.createToken = createToken;\nfunction createTokenInternal(config) {\n    const pattern = config.pattern;\n    const tokenType = {};\n    tokenType.name = config.name;\n    if (!(0, isUndefined_1.default)(pattern)) {\n        tokenType.PATTERN = pattern;\n    }\n    if ((0, has_1.default)(config, PARENT)) {\n        throw (\"The parent property is no longer supported.\\n\" +\n            \"See: https://github.com/chevrotain/chevrotain/issues/564#issuecomment-349062346 for details.\");\n    }\n    if ((0, has_1.default)(config, CATEGORIES)) {\n        // casting to ANY as this will be fixed inside `augmentTokenTypes``\n        tokenType.CATEGORIES = config[CATEGORIES];\n    }\n    (0, tokens_1.augmentTokenTypes)([tokenType]);\n    if ((0, has_1.default)(config, LABEL)) {\n        tokenType.LABEL = config[LABEL];\n    }\n    if ((0, has_1.default)(config, GROUP)) {\n        tokenType.GROUP = config[GROUP];\n    }\n    if ((0, has_1.default)(config, POP_MODE)) {\n        tokenType.POP_MODE = config[POP_MODE];\n    }\n    if ((0, has_1.default)(config, PUSH_MODE)) {\n        tokenType.PUSH_MODE = config[PUSH_MODE];\n    }\n    if ((0, has_1.default)(config, LONGER_ALT)) {\n        tokenType.LONGER_ALT = config[LONGER_ALT];\n    }\n    if ((0, has_1.default)(config, LINE_BREAKS)) {\n        tokenType.LINE_BREAKS = config[LINE_BREAKS];\n    }\n    if ((0, has_1.default)(config, START_CHARS_HINT)) {\n        tokenType.START_CHARS_HINT = config[START_CHARS_HINT];\n    }\n    return tokenType;\n}\nexports.EOF = createToken({ name: \"EOF\", pattern: lexer_public_1.Lexer.NA });\n(0, tokens_1.augmentTokenTypes)([exports.EOF]);\nfunction createTokenInstance(tokType, image, startOffset, endOffset, startLine, endLine, startColumn, endColumn) {\n    return {\n        image,\n        startOffset,\n        endOffset,\n        startLine,\n        endLine,\n        startColumn,\n        endColumn,\n        tokenTypeIdx: tokType.tokenTypeIdx,\n        tokenType: tokType\n    };\n}\nexports.createTokenInstance = createTokenInstance;\nfunction tokenMatcher(token, tokType) {\n    return (0, tokens_1.tokenStructuredMatcher)(token, tokType);\n}\nexports.tokenMatcher = tokenMatcher;\n//# sourceMappingURL=tokens_public.js.map"]},"metadata":{},"sourceType":"script"}